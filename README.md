# 보건복지부의 '국민건강영양조사' 2017,2018년 조사 자료 Data를 기반으로 한 한국인의 영양제 섭취 예측 모델

2020년 1학기 데이터 마이닝 수업 프로젝트

contributed by [ZuseongZIN](https://github.com/ZuseongZIN)

## 1. Introduction

### 1.1 Reseach Background and Motivation (과제 수행 배경 및 동기)

 2019년 30대 제약회사 매출 대부분이 5% 넘게 증가하였지만 유한양행 사의 영업이익은 8% 이상 감소하였다. 꾸준히 매출에 기여했던 비리어드 간염 치료제의 특허가 만료되면서 약값이 인하되었고, 유한양행 사의 원료의약품 매출이 부진하게 되었다. 그 결과 유한양행 사는 신제품 개발에 대한 필요성이 대두되었다. 연구 및 개발(R&D) 비용을 740억원에서 969억원으로 30% 증가시킴으로써 영업이익이 75%나 줄게 되었다. (영업비용= 판매관리비용 + 감가상각비용 + 연구개발비용) 이러한 영업이익의 감소 만회를 위해, 기존 제품의 고객층 확대 및 신제품의 판매를 위해 효과적인 전략을 수립할 수 있는 데이터와 모델이 필요하게 되었다. 이 때, 모델과 모델의 결과는 어떤 조건 때문에, 또는 어떤 고객군을 대상으로 제품을 판매해야 되는지에 대한 근거를 제공해야 한다. 확보한 데이터는 국민영양조사에 기반한 것이므로 데이터에 적합한 식이보충제 제품군을 target으로 삼고 Data mining을 시행할 것이다.
 
### 1.2 Nature and scope of the problem (문제 정의 및 도출)

 앞서 보았듯 현재 유한양행 사의 문제는 원료의약품 매출 부진으로 인한 영업이익 감소이다. 영업이익의 감소 만회를 위해, 앞서 보았던 문제로부터 도출된 Data Science Problem은 기업의 입장에서 국민건강영양조사 데이터 중 어떤 attribute를 선정해 판매전략을 위해 활용해야 하는 지 알 수 없다는 점이다.
 
### 1.3 Previous works and their problems (기존 연구 및 기술의 문제점)

 #### 1) 전문가 시스템
 
 전문가시스템은 인공지능의 한 분야로서 특정 분야에서 전문가의 축적된 지식과 경험을 시스템화하여, 필요할 때 사용하도록 하는 소프트웨어이다. 이는 1960년대 화합물의 구조를 추정하기 위한 DENDRAL 시스템이 시초가 되어, 이후 의료진단시스템인 MYCIN, 기계고장 진단, 손해배상 판정 등 산업계의 전 분야에서 광범위하게 응용되었다. 그러나 이러한 방식은 기업환경의 급속한 변화에 적용하는 데에 많은 비용이 들었고, 시스템을 통해 해결하고자 하는 문제의 영역이 점차 확대되고 복잡해짐에 따라 해당 분야에 통달한 전문가를 찾기가 어려워지면서 지식 획득에 장기간에 걸쳐 많은 비용을 투자해야 하는 등, 여러 문제를 가지고 있었다.
 
 #### 2) 일반 질의(query)
 
 일반 질의(query)는 데이터베이스의 데이터를 분석하는 데 도움이 되는 도구이다. 쿼리 작성, 쿼리 편집, 검색, 찾기, 보고 및 요약 기능을 제공한다. 쿼리는 데이터베이스에 정보를 요청하여 사실로서의 질문에 대한 사실로서의 답변을 제공받는다. 한편 이러한 쿼리는 실제로 사용하려면 사용자가 원하고자 하는 정보가 무엇인지를 정확하게 알아야 하며, 새로운 목적을 찾아야 하고, 무엇을 찾아야 할 지 모호한 상태일 때에는 사용하기 어렵다는 단점이 있다.

#### 3) 사회과학적 연구

 사회과학적 연구는 분석가의 인사이트가 굉장히 중요하다. 특히 국민건강영양조사의 경우 사용하는 속성들이 굉장히 많기 때문에 데이터 마이닝 기법을 사용하지 않으면 분석가의 주관으로 중요할 것 같은 속성들을 추려 내야 한다. 이렇게 하면 만약 분석가가 추려내지 않은 속성들 중에서 중요한 변수가 있을 수도 있기 때문에 타당성이 떨어진다. 

### 1.5 Purposed Method (제안하는 방법의 개요) 

 기본적으로 우리가 사용하는 국민건강영양조사의 Data set은 그 record 수가 충분하나 결측치가 존재하고 불필요한 attribute가 존재한다. 이에 따라 측정된 변수들의 선형 조합(Linear Combination)에 의해 대표적인 주성분을 만들어 차원을 줄이는 PCA 기법, 파레토 원리 기반 결측치 제거 그리고 휴리스틱에 기반해 속성을 제거하는 등의 차원 축소 기법을 통해 Dimension Reduction을 진행하였다. 또한 그룹화, 정규화, 이상치 탐지 및 형변환을 통해 Feature Engineering을 수행하였다.  
 이렇게 전처리 된 데이터를 기반으로, 의사 결정 나무(Decision Tree)와 랜덤 포레스트(Random Forest) 기법을 통해 모델링을 수행하였으며, 이후 Confusion Matrix를 통해 해당 모델의 Accuracy를 평가하였다.

### 1.6 Principal results (논문 주요 결과의 개요)

실세계의 모델링에서 가장 적합한 입력만을 선택하는 것은 시스템 성능에 많은 영향을 미친다. 일반적으로 입력변수의 효과적인 선택은 시스템 차원의 감소나 특징추출 등 다양한 용도로 이용된다. 그러나 많은 입력변수들 중에서 모델에 얼마나 많은 또는 어느 입력들이 필요한지 알 수 없으며, 이는 입력차원이 증가할수록 더욱 더 심각하다. 불필요한 입력들은 학습을 복잡하게 하고 과학습 등에 따른 학습성능의 저하도 가져올 수 있다. 입력변수의 잘못된 선택에 여러 가지 문제들이 발생될 수 있다. 먼저, 입력차원의 증가에 따른 계산시간과 메모리의 증가, 다음으로 요구되지 않는 입력들에 의한 학습의 어려움, 추가적인 요구되지 않는 입력에 의한 비수렴과 모델의 정확성의 저하, 그리고 복잡한 모델에 따른 해석의 어려움 등의 제약이 있다.

## 2. Method

### 2.1 전처리

2.1.1 Data Quality

기본적으로 국민건강영상조사(이하 국건영)에서 원시자료를 기본 Dataset으로 한다. Data object의 양은 어느 정도 충분하다. 2018년 자료는 7992개의 Object와 736개의 attribute를 가지고 있고 2017년 Dataset은 8127개의 Object와 834개의 attribue를 가지고 있다. 모델 디자인에 있어서 더 좋은 Dataset을 찾기 위해 2018년 단독 Dataset과 2018년과 2017년 Dataset을 합쳐서 새로운 Dataset를 만들었다. 2018년과 2017년 Dataset의 attribute가 차이가 있었다. 2018년을 기준으로 하고 2017년에만 조사한 Attribute의 경우 모두 제거했다. 최대한 2018년 Dataset과 비슷하게 하면서 Object의 양을 늘리기 위해서이다. 2017년의 LF_safe변수가 소문자로 저장돼서 2018년의 LF_SAFE와 합쳐지지 못하는 문제가 발생했다. CSV 파일로 Dataset을 추출하여 엑셀 내에서 이를 결합해줬다. 국건영 자료는 설문조사를 기반으로 생성됐다. 그렇기 때문에 결측치가 존재한다. 

#### 2.1.2 Dimension reduction

우선 비해당, 모름, 무응답의 경우 결측치와 다름이 없다고 판단하여 이를 모두 결측치 처리를 해줬다. Data의 양이 충분하다고 판단해서 결측치를 모두 제거하는 것을 최우선으로 생각했다.

#### 2.1.2-1 휴리스틱 기반 속성 제거

주관적으로 Attribute를 제거하는 것은 위험하다. 특히 건강 관련 조사의 경우 의학적인 설문이 많기 때문에 속성을 제거하는 것을 지양했다. 속성 중 가중치에 대한 속성은 모두 제거했다. 가중치는 같은 기에 조사한 자료에 적용하는 것이 아니라 서로 다른 조사 기수 데이터를 통합할 때 사용하는 변수이다. 이 프로젝트에서는 제7기 데이터만을 사용하기 때문에 변수를 삭제해줘도 지장이 없다.
청소년의 경우 경제적 능력이 없어 구매대상으로 적합하지 않다고 판단하여 age를 기준으로 만 18세 미만의 Object를 제거해줬다. 이와 관련하여 소아, 청소년 관련된 질문 항목들을 제거했다. 또한 주관식 설문 문항의 경우 Decision Tree를 사용하기에 적합하지 않기 때문에 제거했다.  
 

#### 2.1.2-2 파레토 원리 기반 결측치 제거

결측치를 처리하는 최적의 해법은 상황과 분석 목적에 따라 달라지기에, 이를 어떻게 처리할 지 결정하는 것은 쉽지 않다. 그럼에도 불구하고 결측치로 인한 정보의 손실이 분석 결과에 미치는 영향을 고려한다면 결측치를 적절하게 처리하는 것은 중요하다. 이에 우리는 데이터에 있는 결측치를 ‘파레토 원리’에 기반하여 제거하고자 하였다. 파레토 원리(또는 희소 인자의 원리)는 전체 결과의 80%는 전체 원인의 20%에서 비롯된다는 것이다. 데이터의 무질서도(Entropy) 또한 20%의 주요 변수들로부터 기인할 것이다. 파레토 원리를 적용한 결측치의 제거 방식은 아래와 같다.

[1] 휴리스틱 기반 속성 제거가 완료된 데이터 셋에서, 행과 열 개수의 각각 20%를 데이터 셋의 임계 개수로 설정한다.
[2] 열을 기준으로 NA값이 많은 20%를 먼저 제거하여, 80%의 유효 열을 남긴다.
[3] 다시 행을 기준으로 NA값이 많은 20%를 제거하여 80%의 유효 행을 남긴다. 이 때 sort()를 통해 NA 값이 많은 순서대로 정렬하고, 반복문 while()을 통해 모든 행에 대해 해당 과정을 수행하게 한다.
[4] 행이나 열의 개수가 임계 개수보다 작아지기 직전까지 [2]와 [3] 과정을 반복한다.
[5] 행보다는 열이 더 가치 있다는 판단 아래, 남아있는 NA값을 행 제거를 통해 처리한다.
2.1.
2-3 PCA

PCA 분석방법은 측정된 변수들의 선형 조합(Linear Combination)에 의해 대표적인 주성분을 만들어 차원을 줄이는 방법이다. 현재 갖고 있는 데이터 중 식품섭취조사 자료인 ‘개인별 24시간 회상자료를 통한 영양소 섭취량에는 식품섭취량, 에너지, 수분 등을 포함하여 총 27가지의 영양소에 관한 연속형 변수들이 있다. 이 27가지의 변수들을 PCA분석을 통해 주요 변수들을 뽑아주었다. 총 27가지의 PC원소들 중 PC4번 변수까지 포함시켰더니 누적분산이 0.7을 넘어 유효한 PC그룹군을 4개까지로 지정하였다. 따라서 27개의 변수를 제거하고 새로운 4개의 PC원소를 추가함으로써 기존 변수들이 갖고 있던 최적의 성질을 유지한 채 차원을 줄여주었다. 이렇게 뽑힌 4가지의 PC원소를 살펴보니 가장 영향력이 큰 PC1변수에 큰 영향을 끼치는 기존 변수들은 인, 에너지, 카로틴으로 인체의 열량 및 활력에 영향을 미치는 요소들로 되어 있었다. 따라서, PC1 변수는 ‘energy’로 재정의 하였다. 두번째 PC2변수는 불포화지방, 지방, 포화지방이 영향력의 비중이 가장 크기 때문에 ‘fat-series’로 재정의하였다. 세번째 PC3변수는 비타민A, 레티놀, 카로틴이 영향을 많이 끼쳤다. 레티놀과 카로틴은 비타민 A의 한 종류이므로 변수를 ‘Vitamin_A’로 재정의하였다. 마지막 PC4변수는 3계지방산, 다가불포화지방산, 6계지방산이 큰 영향을 끼쳐서 ‘fatty-series’로 재정의하였다. 이처럼 27개의 연속형 변수들을 PCA분석을 통해 기존 분산을 가장 잘 반영하는 축으로 사영하여 주요변수 4개를 뽑아주어 차원을 축소시켜주었다.
 
![image](https://user-images.githubusercontent.com/44190559/120962731-47990100-c79b-11eb-8a1f-3c0d21891b12.png)


### 2.1.3 Feature engineering
 #### 2.1.3-0 EDA
 어떤 Feature가 핵심적인 역할을 할 수 있을지 직관에 따라 선택하고 Target Attribue와의 연관성을 알아보기 위해 시각화 해보았다.


![image](https://user-images.githubusercontent.com/44190559/120962762-52539600-c79b-11eb-9970-50d1cdf294e9.png)
![image](https://user-images.githubusercontent.com/44190559/120962772-54b5f000-c79b-11eb-826d-ac06a78921da.png)

	
그림 1 : 가족크기와 영양제 복용여부		그림 2 : 성별과 영양제 복용여부


![image](https://user-images.githubusercontent.com/44190559/120962793-60091b80-c79b-11eb-9e1f-e48e7f7983f6.png)
![image](https://user-images.githubusercontent.com/44190559/120962797-61d2df00-c79b-11eb-82b5-b64183231acf.png)


         
그림 3 : 결혼여부와 영양제 복용여부		그림 4 : 기초생활수급과 영양제 복용여부


![image](https://user-images.githubusercontent.com/44190559/120962804-67c8c000-c79b-11eb-92a4-5bf83a9984a0.png)
![image](https://user-images.githubusercontent.com/44190559/120962811-6a2b1a00-c79b-11eb-8cfc-5d3c04f8aeb9.png)


         
그림 5: 치간칫솔과 영양제 복용여부		그림 6: 소득분위와 영양제 복용여부


 이렇게 Target Attribute와 직관적으로 관련이 있어 보이는 Attribute들은 여섯 개의 그림은 Modeling 과정에서 실제로 연관이 있는지 검증될 것이다.
#### 2.1.3-1 나이 그룹화
고객군 대상이 될 수 없는 20세 미만의 미성년을 제외한다 하더라도 20세 이상부터 부터 가장 나이가 많은 80살의 사람까지 연속형의 변수는 나름 그 활용성이 높다고 생각하여 나이의 특성을 좀 더 활용하기 위해 그룹을 지어주었다. 20세부터 30세까지의 226명의 사람들은 ‘young’그룹으로 묶어주었고, 그 위로부터 65세 아래까지의 총 2218사람들은 middle로, 그 위의 521명의 사람들은 ‘old’그룹으로 묶어주었다. 따라서 연속형의 나이 변수와 더불어 명목형의 age_group변수를 추가하여 데이터의 특성을 보강했다.

#### 2.1.3-2 지역 그룹화
나이의 그룹화와 더불어, 지역 또한 17개의 변수들로 나뉘어져 있다. 따라서 그 특성을 좀 더 세분화하고 지역의 특성을 데이터 내에서 좀 더 활용하기 위해, 17개의 지역 중 경기/서울/인천/충북/충남/세종/대전 지역을 합쳐서 윗지방을 의미하는 1로, 그 아래의 강원/전북/전남/광주/제주/경북/경남/대구/부산/울산 지역을 아랫지방을 의미하는 0으로 합쳐주었다. 이 이산화는 명목형 변수들을 큰 틀로 묶어주어 데이터에 대한 특정화를 보장하며 soft한 스플릿을 가능하게 해준다.
#### 2.1.3-3 Normalization
국민건강영양조사 원시 자료에서 애트리뷰트 타입이 등간 속성이나 비율 속성인 경우 변화량의 통일을 위해 데이터를 정규화를 해줄 필요가 있다. 예를 들어 나이와 소득 같은 경우 똑같이 10이 증가했다고 같은 의미를 가지지 않는다. 두 개의 데이터 객체가 있다고 가정해보자. 두 데이터 객체의 나이차가 20살이 나고 소득 차이가 2천만원이 나는데 분석을 할 대 이 변화량을 절대적인 크기로 해석하면 오류를 범할 가능성이 높다. 그 이유는 두 데이터 타입이 가질 수 있는 값의 범위가 다르기 때문인데, 값의 범위를 통일시켜 주는 게 바로 정규화라고 할 수 있다. 정규화의 종류는 MinMax Normalization, Robust Normalization, Standardization 등 굉장히 많지만 우리는 그 중에서 수업시간에서 다룬 MinMax Normalization을 채택했다. MinMax Normalization은 자신의 값과 최소값의 차를 최대값과 최소값의 차로 나눠준 값을 의미한다. 이렇게 하면 정규화를 거친 데이터에 한해서 범위가 0~1로 고정되기 때문에 속성이 다른 데이터끼리 변화량이 같은 의미를 가지게 된다.

#### 2.1.3-4 이상치 탐지
국민건강영양조사 데이터를 분석해서 다른 데이터와 비교했을 때 많이 벗어나 있는 데이터, outlier를 찾아서 제거해내야 한다. 앞에서 전처리한 데이터를 보았을 때 대부분의 데이터는 범주형 데이터이다. 전처리를 통해 범주형 데이터에서 NULL 값이나 모름, 무응답을 전처리 했으므로 연속형 데이터에 이상치가 있을 가능성이 크다. 따라서 대표적인 연속형 어트리뷰트인 1) 월평균 가구 총소득, 2) 체질량 지수 이상치를 제거한다.
1) 월평균 가구 총소득: 먼저 월평균 가구 총소득에 대한 이상치를 탐색하고 시각화해야 한다. 그러나 애초에 원시 데이터에서 소득이 월 평균 1500만원 이상인 사람들은 1500만원으로 바꿔줌으로써 이미 이상치 처리가 되어있었다.
2) 체질량지수: 데이터 표본에 포함된 사람 중에서 정말로 키가 극단적으로 크거나 작고 몸무게가 극단적으로 크거나 작은 사람들이 있을 것이다. 체질량지수에서 이상치를 찾아내서 이런 사람들에 대한 데이터를 한꺼번에 제거할 수 있을 것이다.  메소드는 boxplot을 이용한다. 
 
 
 ![image](https://user-images.githubusercontent.com/44190559/120962828-731beb80-c79b-11eb-87fe-2a3b5d4a34d9.png)
 
 
위 그림을 보면 상자의 크기는 제3사분위수에서 제1사분위수를 나타내게 된다. 이 길이를 L이라고 하자. 상자 위아래의 선분은 최소값, 최대값을 표시하며 1.5L을 넘어가는 부분을 이상치로 표시하게 된다. 이상치가 위 아래로 존재하므로 삭제해야 한다.

![image](https://user-images.githubusercontent.com/44190559/120962847-7b742680-c79b-11eb-9c63-b110da2bad9d.png)

 이상치의 대략적인 양을 알아내기 위해서 3D scatter를 통해 3차원으로 시각화를 해보면 체질량 지수가 대략 0(15)에 가까운 사람과 체질량 지수가 1.0(41)에 가까운 사람이 존재한다. 위로 41 * 0.8 = 32.8 이상의 체질량 지수를 가진 사람들을 제거해준다. 마찬가지로 아래로는 체질량 지수가 15에 가까운 순으로 10개 정도를 제거해준다.


#### 2.1.3-5 형변환
설문 내용을 데이터 수집가가 스프레드시트에 입력할 때 데이터 타입은 캐릭터형 또는 숫자형 밖에 없다. 하지만 설문 문항은 대부분이 범주형 데이터이다. 원시 데이터에서 확인해봤을 때 문자로 입력된 것들만 범주형 데이터로 분류되었고 나머지는 숫자형 데이터였다. 우리가 쓰는 모델은 분류나무 모델이기 때문에 범주형 데이터로 변환해줘야 할 것은 범주형 데이터로 변환해줘야 한다. 범주형 데이터로 변환하지 않았을 경우에 의사결정나무에서 분류를 할 때 split condition이 어떤 숫자 이상/이하로 설정된다. 예를 들어 Factor level이 3(1, 2, 3)인 경우 split condition 기준이 2.78이 될 수 있다. 이렇게 해도 모델은 잘 돌아가지만 올바르게 분석되었다고 보기는 어렵다. 원시 데이터에서 대부분의 형변환은 int->factor였고 형변환을 할지 말지는 우리의 주관에 따랐다.

앞으로 기술하는 기법들을 적용한 후 is.na() 함수를 이용하여 결측치가 500개 미만이 될 때까지 반복했다. 더 이상 열을 지우지 않고 NA이 하나라도 있는 행을 삭제해 Dataset을 완성했다.

### 2.2 Exploratory data analysis 
#### 2.2-1 Cluster Analysis
모델링에 들어가기 앞서 데이터 자체, 즉 고객들이 갖고 있는 자연스러운 특징들(군집, 연관성 등)을 알아보기 위해 2가지 비지도 학습적 데이터 탐색을 시도하였다. 
첫 번째로는 군집 분석을 시도하였다. 군집분석을 하는 이유는 데이터마이닝 강의의 내용을 요약해보면 동일한 성격을 가진 여러 개의 그룹으로 데이터를 분류하기 위해 사용한다. 대상 개체를 유사하거나 서로 관련 있는 항목끼리 묶어서 몇 개의 집단으로 그룹화하거나, 각 집단의 성격을 파악해서 전체의 구조에 대한 이해를 돕는다. 군집분석은 종속변수에 대한 독립변수의 영향을 분석한다. 사전에 정의된 특수한 목적은 없다. 데이터 자체에 의존해서 데이터 구조와 자료를 탐색하고 요약하는 기법이다.
##### 1)	국민건간영양조사 데이터와 같이 방대한 데이터의 경우에는 전체에 대한 의미 있는 정보를 얻어낼 수 있다. 
전체를 유사한 성질을 지니는 군집으로 구분한다면 군집에 대한 특성을 분석하고 이를 통해 전체 데이터에 대한 직관을 얻고 활용할 수 있을 것이다. 이를 통하여 고객들을 자연스럽게 군집화 하고, 모델링과 평가가 끝나고 실전 배치 단계에서, 해당 군집이 갖고 있는 특성들을 활용하여 더 다양한 고객화 전략 및 마케팅이 가능해진다. 
##### 2)	우리가 시도할 군집분석은 수업 시간에 배운 Hierarchical Clustering, K-means clustring이다.
계층적 클러스터링은 병합적 방법과 분할적 방법을 바탕으로 군집을 형성시킨다. 계층적 클러스터링은 병합적 방법과 분할적 방법을 바탕으로 군집을 형성시킨다. 한 관찰단위는 한 군집에 속하면 다른 군집에는 다시 속할 수 없다. 군집은 덴드로그램(Dendrogram)이라는 도표를 이용해 군집들 간의 관계를 파악한다.

##### 3)	k-means 클러스터링은 사전에 결정된 군집 수 K에 기초하여 전체 데이터를 비슷한 k개의 군집으로 구분하는 방법이다. 상호배타적인 k개의 군집을 형성함으로써 군집들을 형성한다.
K-means Clustering 방법에 있어, 최적의 Centroid를 찾기 위해 일반적으로 사용되는 방법론을 적용하였다. 즉, 초기 Centroid를 random하게 잡은 후, Euclidean distance를 이용해 Closeness를 비교하는 Matrix를 새로 구성한 후, 이를 기반으로 새로운 Centroid를 구성하고 이 과정을 반복적으로 수행하게끔 함수를 만들었다. 새로운 함수를 정의, 선언하여 사용될 수 있게 하였고 이후 kmeans() 등의 외부 라이브러리를 사용해서도 동일 과정을 수행할 수 있었다. 

##### 4)	군집분석을 위해 어떤 전처리가 필요한가? 
가장 우선적으로 k의 개수를 정하는 것이 필요하다. 상관관계분석을 통해 데이터의 이상치를 살펴보고 제거한다. 데이터의 범위가 맞지 않아 정규화를 시켜야 했다. 데이터에서 목적에 가장 부합하는 Atrribute를 선정했다. 결과적으로 활용가능한 총 속성은 나이, 성별, 월평균소득, 결혼여부, 신장, 체중, 체질량지수, 식품섭취량, 에너지, 수분, 단백질, 지방, 포화지방산, 단일불포화지방산, 다가불포화지방산, n-3계 지방산, n-6계 지방산, 콜레스테롤, 탄수화물, 식이섬유, 당, 칼슘, 인, 철, 나트륨, 칼륨, 비타민A, 카로틴, 레티놀, 리보플라빈, 나이아신, 비타민C로 32개가 있다. 여기서 영향이 중복되는 변수는 제거하고 필요하다면 주성분추출을 시행한다.

두 번째 방법은 연관성 분석이다. 연관성 분석을 위해 갖고 있는 데이터셋들을 binarization 및 discretization을 사용하여, 이산적인 transaction data set으로 변형하여 연관성 분석을 시행하였다. 이 또한 모델링이 끝난 후 제품 판매 전략시의 인사이트를 도출할 수 있는데, 예시로 데이터셋에서 칫솔을 사용하고, 아침식사를 하는 고객들은 식생활 형편이 좋다는 연관성을 도출할 수 있었다. 따라서 이러한 연관성을 통하여 더욱 다양한 마케팅 전략을 펼칠 수 있고, 더욱이 데이터들의 속성을 더욱 자세히 조사함으로써 데이터에 대한 이해를 더욱 높일 수 있다는 장점이 있다. 따라서 분류 분석이나 연관성 분석 같은 탐색적 데이터 분석 방법은 당장 직접적으로 모델링의 성과를 높이거나, 데이터셋의 정확도를 향상시키는 효과를 불러올 수는 없지만, 데이터를 탐색함으로써 business problem의 식이 보충제의 판매전략을 다양화할 수 있다. 

#### 2.2-2 Association Rule Discovery
 규칙(rule)이란 ‘if A(조건) then B(결과)’의 형식으로 표현된다. 연관규칙은 특정 사건이 발생하였을 때 함께 발생하는 또 다른 사건의 규칙을 말한다. 연관분석은 이러한 연관 규칙을 찾아내는 분석 기법이다. 일반적으로 추천 상품 카테고리를 만들고자 할 때 많이 쓰인다. 우리의 목표는 영양제를 먹는 사람이 어떤 사람들인지를 찾는 것이기 때문에 ‘if A then B’에서 A 또는 B가 영양제를 먹는 사람이라면 이에 연결된 변수가 영양제를 먹는 사람들과 어느정도 연관이 있다고 생각할 수 있다. 하지만 연관분석은 비지도 학습이기 때문에 타겟 변수를 따로 설정할 수는 없다. 따라서 연관분석을 실시하는 이유는 타겟 변수와 연관성이 있는 변수를 찾는 것이 아니라 온전히 변수들끼리의 연관성을 파악하기 위함이다. 이를 통해 연관성이 높은 변수들끼리 하나의 변수로 합치는 feature engineering을 추가적으로 실시할 수 있다. 
 연관분석을 진행하기 위해 원래 데이터를 거래 데이터 형식으로 변환시켜줘야 한다. 우리는 파레토 법칙에 따라 전처리가 완료된 데이터를 먼저 이진화 시켜줬다. 연속형 변수 또는 factor level이 많은 변수들은 우리의 주관적 판단에 따라 과감하게 0과 1로 나누었다. 거래 아이디와 아이템 항목을 속성으로 가지는 빈 데이터프레임을 만들고 이진화된 데이터를 거래 테이블에 넣어주었다. 다시 말해 이진화된 데이터의 행 번호는 거래 데이터의 거래 아이디에 대응되고, 이진화된 데이터의 각 행에서 1로 표시된 데이터는 모두 거래 데이터의 아이템 항목에 담긴다.
 연관규칙을 생성하는 알고리즘으로 Apriori 알고리즘을 썼다. 지지도와 신뢰도에 따라 규칙이 달라지기 때문에 많은 시도를 했다. 신뢰도는 최소 0.7 이상으로 설정했으며 지지도는 디폴트값인 0.1보다 크게 설정하였다. 연관 규칙을 생성할 때 특정 속성을 제외시킬 수 있어서 분석 결과에서 당연한 규칙들은 제외시켰다. 예를 들어 분석을 진행하면서 ‘점심을 먹는다 -> 영양제를 먹는다’와 같은 규칙이 나왔었는데 점심을 먹지 않는 사람은 많이 없을 것으로 예상되기 때문에 ‘점심을 먹는다’는 제외시켰다. 
### 2.3 Modeling

#### 2.3.1 Decision tree
의사 결정 나무(Decision tree)는 데이터를 분석하여 이들 사이에 존재하는 패턴을 예측 가능한 규칙들의 조합으로 나타내며, 그 모양이 나무와 같아 의사결정나무라 불린다. 데이터 마이닝 과정에서 일반적으로 사용되는 방법론으로, 입력 변수를 바탕으로 목표 변수의 값을 예측하는 것을 생성하는 것을 목표로 한다. 결정 트리를 구성하는 알고리즘에는 주로 하향식 기법이 사용되며, 각 진행 단계에서는 주어진 데이터 집합을 가장 적합한 기준으로 분할하는 변수값이 선택된다. 서로 다른 알고리즘들은 분할의 적합도를 측정하는 기준이 있으며, 이 과정에서 지니 불순도, information gain, classification error 등이 사용된다. 우리는 rpart, caret 등의 library를 통해 Decision tree를 구성하였으며, Confusion matrix를 통해 DT의 accuracy를 평가하였다.

#### 2.3.2 Random Forest
Random Forest는 Decision Tree 기반의 예측 모델이다. 무작위의 의사결정트리를 여러 개 만들어서 이로부터 나온 예측 결과들의 평균 또는 다수의 예측 결과를 이용하는 앙상블 기법을 이용한다. 많은 개수의 트리는 보다 높은 성능의 모델을 보장하지만 계산시간이 오래 걸릴 수 있다. 따라서 적절한 개수의 트리를 만들고 사용할 변수의 개수를 정해야 한다.  의사결정나무와 마찬가지로 지니계수, information gain, 예측오차 등을 통해서 트리가 만들어진다. 하지만 랜덤포레스트의 결과물은 위에서 설명했듯이 다수의 의사결정 나무의 결과물들을 앙상블 기법을 이용해 평균을 계산한다. 이 때문에 트리에 대한 직접적인 시각화가 불가능하다. 따라서 랜덤포레스트 모델을 통해서 단지 정확도와 confusion matrix를 계산해보고 전에 만든 Decision Tree보다 좋은 모델이 존재하는지 확인을 하는 용도로 모델을 사용할 것이다.

## 3. Results
### 3.1 전처리 결과
 
 ![image](https://user-images.githubusercontent.com/44190559/120962865-875fe880-c79b-11eb-9516-c644c089e9cf.png)
 
전처리는 위에서 기술한 Method를 그대로 적용했다. raw데이터는 분석 대상에서 제외되는 행, 즉 18세 미만 청소년 행과 휴리스틱 기반 속성 제거를 한 결과이다. Processed1부터 processed5까지는 파레토 법칙에 기반한 Nan값 처리 과정에 대한 결과이고, 마지막 processed6은 Feature engineering을 적용한 결과이다.  

### 3.2 Cluster Analysis 결과
#### 3.2-1 상관분석 : 
전처리리를 마무리 했던 데이터에서는 연속형 Attribute가 많지 않았다. 그래서 추가적인 군집분석을 시행하기 위해서 CRISP-DM의 과정을 따라 다시 데이터 전처리로 돌아갔고 연속형 Attribute를 찾아보았다. 연속형 Attribute 간에 상관이 높은 Atrribute를 분석하고 제거하였다. 

![image](https://user-images.githubusercontent.com/44190559/120962877-8d55c980-c79b-11eb-8f44-a56e119f5e02.png)
 
예시1 : N_FAT(지방)과 N_MUFA(단일불포화지방산)은 상관관계가 높으므로 N_FAT하나만 군집분석에 사용한다. 
 
 ![image](https://user-images.githubusercontent.com/44190559/120962895-95156e00-c79b-11eb-9b46-4f7940c682ae.png)
 
예시2: HE_wt (몸무게)와 HE_BMI (체질량지수)는 상관관계가 높으므로 HE_BMI 하나만 군집분석에 사용한다.
#### 3.1-2 최적 군집 수의 파악
  Nbclust 라이브러리를 통해 데이터 속에서 최적의 군집 수를 파악할 수 있다. 분석 결과, 4에서 6까지의 군집 수가 최적 군집수로 파악되었다. 따라서 4에서 6사이의 수를 군집 개수로 지정하여 군집분석을 시행한다. 

![image](https://user-images.githubusercontent.com/44190559/120962908-9c3c7c00-c79b-11eb-9765-f257facfd8bc.png)
![image](https://user-images.githubusercontent.com/44190559/120962915-9fd00300-c79b-11eb-811c-179861697156.png)

![image](https://user-images.githubusercontent.com/44190559/120962928-a8c0d480-c79b-11eb-9141-2f115f85bdf1.png)
 
K-means를 사용한 분할적 군집분석(Partitional Clustering)이 정당한 군집으로 나뉘었다는 근거를 얻기 위해서 계층적 군집분석(Hierarchical Clustering)을 추가적으로 시행하였다. 이는 단지 초기 군집을 잡을 때 사용한 centroid들과 결과로써 나온 군집들이 타당한 군집인지를 판별하기 위해 사용하였다. 계층적 군집 분석은 너무 많은 양의 데이터를 포함하기 어렵기 때문에 대표적으로 100개 정도를 랜덤하게 뽑아서 사용하였다.

#### 3.1-3 K-means Cluster Analysis
직관적으로 군집이 존재할 만한 데이터끼리 군집분석을 시행해보았다.
 
 
 ![image](https://user-images.githubusercontent.com/44190559/120962947-b24a3c80-c79b-11eb-95e9-c9dcc7f6efcd.png)
 
그림 : 나이, 소득분위에 대한 군집분석

 ![image](https://user-images.githubusercontent.com/44190559/120962955-b9714a80-c79b-11eb-9f22-7856595e3e29.png)
 
그림 : 비타민A와 비타민 C에 대한 군집분석

![image](https://user-images.githubusercontent.com/44190559/120962972-c0985880-c79b-11eb-9d57-0e1fcdbda655.png)
 
그림 : 몸무게와 비타민에 대한 군집분석

![image](https://user-images.githubusercontent.com/44190559/120962983-c68e3980-c79b-11eb-9733-194e4368caed.png)
 
그림 : 칼슘과 나트륨대한 군집분석


![image](https://user-images.githubusercontent.com/44190559/120963004-cd1cb100-c79b-11eb-8cf8-459a87e79e90.png)
 
그림 : 철과 인에 대한 군집분석

 
 ![image](https://user-images.githubusercontent.com/44190559/120963021-d279fb80-c79b-11eb-957f-931357d1c25c.png)

그림 : 나이와 칼슘에 대한 군집분석


![image](https://user-images.githubusercontent.com/44190559/120963041-db6acd00-c79b-11eb-91ed-4caf2167279b.png)
 
그림 : 월 평균 소득과 식품섭취량에 대한 군집분석

 ![image](https://user-images.githubusercontent.com/44190559/120963050-e0c81780-c79b-11eb-9005-e37c7b303729.png)
 
그림 : 나이와 콜레스테테롤에 대한 군집분석

![image](https://user-images.githubusercontent.com/44190559/120963063-e6bdf880-c79b-11eb-823d-48f62e72e9a4.png)
 
그림 : 설탕과 탄수화물에 대한 군집분석

또한 3차원 형태로도 시각화를 수행하였다. 3개 Attribute 이상, 즉 3차원 이상의 데이터에 대해 군집을 분석할 때 2차원 상에서는 직관적으로 이를 구분하기 어렵다. 따라서 이를 3차원 이상으로 새로이 시각화할 수 있는 방법이 필요했다. 우리는 K-means clustering에 있어 kmeans() 라이브러리 내장 함수를 이용하여 clustering을 수행하였고, clustered 된 데이터를 scatterplot3d라는 3차원 시각화 라이브러리를 통해 시각화할 수 있었다. 아래는 나이(datas2$age), BMI(datas2$HE_BMI), 월간 소득(datas2$earn_month)의 세 가지 attribute를 통해 군집을 시각화한 예시이다.
 
 ![image](https://user-images.githubusercontent.com/44190559/120963078-ede50680-c79b-11eb-9e2e-0cbdb44f7c41.png)
 
그림 : 나이, 체질량지수, 월평균소득에 대한 군집분석 시각화(2차원)
위처럼, 다양한 군집을 2차원 평면 상에서 표현할 경우 이를 제대로 구분할 수 없고, x,y,z의 세 개 축을 각각 원하는 Attribute로 지정하여 군집이 3차원 상에서 어떻게 형성되는 지 확인할 수 있었다.
 
 ![image](https://user-images.githubusercontent.com/44190559/120963103-f3dae780-c79b-11eb-9bc7-d7fc3c2384a5.png)
 
그림 : 나이, 체질량지수, 월평균소득에 대한 군집분석(3차원)
#### 3.2-4 Multiple runs
갖고 있는 데이터들 중 clustering하기에 적합한 연속형 데이터를 전처리하고 통합하니, 나이, 월 소득, 몸무게, 키, 전 일 섭취 영양소들(특히 PCA를 통해 나온 PC1요소인 make energy series: [에너지, 단백질, 인]의 요소를 대표로 사용하였다.) 등이 사용하기 좋은 형태로 나와 있어서, 정규화를 통해 k-means clustering을 다시 시행하였다. 
분할적 군집분석(partitioning clustering)을 실시한 이유는 계산비용 측면에서 초반에 다양한 시도를 하기 위해서, 특히 객체 별로 exclusive하게 어떤 군집에 속할지를 명시적으로 할당하기위해 hard clustering을 해주었다. Fuzzy나 가중치를 활용한 soft clustering은 비용적인 문제로 선택하지 않았고, 다만 결과로 나온 군집들을 평가하기 위해, 그래프를 활용하여 단일 연결을 기반으로 한 응집형 계층적 군집화를 통하여, 그 합리성을 판단하였다. (계층적 군집 분석은 ‘average’ 를 기반으로 거리계산을 하였다.) 밀도나 분포 기반의 군집화는 데이터 셋의 형태에 최적화된 기법이 아니라 판단되어 평균(Mean)이나 중앙값(Median)을 기준으로 잡은 최소 거리를 기준으로 clustering을 하였다. 
가장 타당한 군집의 개수는 “fviz_nbclust” 함수를 통하여 정해주었는데, 이는 Intra-cluster 거리는 최소화하면서, Inter-cluster 거리를 최대화해주는 군집의 개수를 알려주는 함수이다. 이후 데이터가 정규 분포를 띄지 않는 특징을 반영하여 ‘pearson’ 상관계수를 사용하여 다시 한번 군집화를 시도하였다. nstart = 25로 설정하여 예측력을 높이고, 초기 centroid가 랜덤하게 잡히다 보니 결과로 나온 cluster들이 상당히 합리적이지 않은 결과들이 종종 도출되기도 하였다. 따라서 적절한 Centroid를 찾아 업데이트하고, ‘좋은 군집’을 찾기 위해 여러 가지의 시도를 할 수 있었다. 
초기 Centroid를 random하게 잡은 후, Euclidean distance를 이용해 Closeness를 비교하는 Matrix를 새로 구성한 후, 이를 기반으로 새로운 Centroid를 구성하고 이 과정을 반복적으로 수행하게끔 함수를 만들었다. 이후 여러 번의 시도를 통해서 초기 centroid를 바꾸면서 군집을 나눌 수 있었다. 
하지만 이 결과는 다양한 그룹들을 가져오긴 하지만 나온 그룹이 정말 근거가 있는 그룹인지를 확인할 수 없고, 다만 분석 목적에 합당한 그룹을 뽑는다는 점에서 근거가 부족하여 기각하였다. 물론 결과로 나온 군집 중에는 정말 합리적인 군집이 있을 수 있지만, 그렇다면 다른 과정을 통해서도 도출 가능하기 때문에, 단지 검증의 용도로만 사용하였다.
       
예시 : 먼저 initial centroid를 기존의 방식으로 도출하기 위해 함수 centroid(x)를 구현하였고, 이를 통해 첫 번째 centroid를 도출한 모습이다.
 
 ### 3.3 연관분석 결과
<아무런 설정을 하지 않은 경우>
 
 ![image](https://user-images.githubusercontent.com/44190559/120963124-fc332280-c79b-11eb-914d-5422690a9284.png)
 
(지지도 = 0.2, 신뢰도 0.8)로 설정해서 모델을 돌린 결과이다. 상위 다섯 개의 규칙은 아래와 같다. 단, ‘점심을 먹는다’, ‘저녁을 먹는다’ 속성은 제외시키고 돌린 결과이다. 

<우변 상수를 ‘영양제 복용’으로 적용한 경우>
 
 ![image](https://user-images.githubusercontent.com/44190559/120963141-02c19a00-c79c-11eb-8d6e-2d2b1d14805f.png)
 
(지지도 = 0.1, 신뢰도 = 0.7)로 잡았다. 우변상수를 결정했을 때 신뢰도가 0.7보다 크면 규칙이 생성되지 않았다. 생성된 규칙은 아래와 같다. BM2_3은 치간칫솔을 사용하는 사람이다. 우변상수를 설정한 경우 신뢰도를 0.7 이상으로 잡으면 규칙이 생성되지 않는다.

#### 4) 의사결정나무 분석 결과  
 
 ![image](https://user-images.githubusercontent.com/44190559/120963158-0a813e80-c79c-11eb-998b-be02f452cb01.png)
 
Model 생성 결과, 위와 같은 모델이 생성되었다. BM2_3 -> 칫솔 외, 치간칫솔의 사용 여부
 
Model의 평가 결과, 해당 Model의 Accuaracy는 66.38%로 측정되었다.

(5) 랜덤포레스트 분석 결과
앞에서 전처리한 데이터는 96개의 Attribute와 2965개의 Record를 가졌다. 계산 시간을 줄이고 데이터셋 마다 모델 출력결과를 확인하기 위해 전체 데이터셋을 4개로 나누었다. 이를 위해 R코드를 작성하여 각 데이터셋을 학습하기 위해 train set과 test set으로 7:3으로 나누었다. 그리고 개별적으로 Train set으로 랜덤 포레스트 모델을 돌려본다. 모델의 파라미터는 1000, 변수의 개수는 6개이다. 모델의 결과는 각각 다음과 같다.
 
 ![image](https://user-images.githubusercontent.com/44190559/120963175-12d97980-c79c-11eb-89b8-da553e1113ad.png)
 
그림 1 : Data1에 대한 모델학습결과

 ![image](https://user-images.githubusercontent.com/44190559/120963195-1967f100-c79c-11eb-8e66-eb9210ede449.png)
 
그림 2 : Data2에 대한 모델학습결과

 ![image](https://user-images.githubusercontent.com/44190559/120963213-1ff66880-c79c-11eb-8f55-91fae47b778f.png)
 
그림 3: Data3에 대한 모델학습결과

 ![image](https://user-images.githubusercontent.com/44190559/120963227-24bb1c80-c79c-11eb-8914-ed04cf66eb9e.png)
 
그림 4: Data4에 대한 모델학습결과
 
 ![image](https://user-images.githubusercontent.com/44190559/120963248-2edd1b00-c79c-11eb-9d3e-38d680513157.png)
 
그림 5: 전체 Data에 대한 모델학습결과
